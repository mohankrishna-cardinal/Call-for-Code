{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Payment Plan Campaign\n\nJupiter Energy is the leading supplier of clean, renewable energy for the greater Boston area, servicing nearly 5 million customers across 4 counties. To better serve their clients, especially those facing financial hardship, Jupiter is launching a new set of electricity rates and payment plans. These new plans will significantly lower the total cost of energy for their clients in need.\n\nYour task is to identify the customers who could benefit from the new plans.\n\nIn this Python 3.10 notebook, you'll improve the quality of the data and then build a machine learning model to determine which clients should be offered the new payment plans because they are likely to miss payments. You\u2019ll be guided through these steps:\n\n- Step 1: Load the data\n- Step 2: Explore the data\n- Step 3: Prepare the data\n- Step 4: Build and train models\n- Step 5: Evaluate the models\n- Step 6: Predict potential missed payments\n- Step 7: Deploy the model (Optional)\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Insert a project token\n\nWhen you import this project from the Gallery, a token should be automatically generated and inserted at the top of this notebook as a code cell such as the one below:\n\n```python\n# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='YOUR_PROJECT_ID', project_access_token='YOUR_PROJECT_TOKEN')\npc = project.project_context\n```\n\nIf you do not see the cell above, follow these steps to enable the notebook to access the dataset from the project's resources:\n\n* Click **More -> Insert project token** in the top-right menu section.\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n\n* This should insert a cell at the top of this notebook similar to the example given above.\n\n  > If an error is displayed indicating that no project token is defined, follow [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data).\n\n* Run the newly inserted cell before proceeding with the notebook execution below."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Import libraries\n\nMany popular open source libraries are pre-installed on Cloud Pak for Data platform environments. All you have to do is import them. If a library is not preinstalled, you can add it through the notebook or by adding a customization to the environment in which the notebook runs."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\n%matplotlib inline\nplt.style.use('ggplot')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 1: Load and access the data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df = pd.read_csv(project.get_file('Historical-Customer-Payments-Prepared.csv'))\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Alternative load data method\n\nUse the **Insert code to cell** function to automatically generate code that loads the data and shows the first 5 rows in a pandas DataFrame:\n1. Click the **Code Snippets** icon on the notebook action bar.\n2. In the side pane, click **Read data**.\n3. Click **Select data from project**.\n4. Cick **Data assets > Historical-Customer-Payments-Prepared.csv**.\n5. Click **Select**.\n6. In the *Load as* drop-down list, select **pandas DataFrame**.\n7. Click **Insert code to cell**.\n8. Rename the dataframe from `df_data_1` to `df` in the second to last link and then run the cell."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# rename df_data_X to df"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 2: Explore the data\n\nYou can use plots, graphs, and summary statistics to systematically go through the data. For example, you can plot the distribution of all variables, plot a time series of the data, transform variables, look at all pairwise relationship between variables using scatterplot matrices, and generate summary statistics for all of them. Here are some of these methods."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Check the summary statistics"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.describe()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Check the target variable `MISSED_PAYMENT`"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "sns.countplot(x=df[\"MISSED_PAYMENT\"])\nplt.show()\ndf[\"MISSED_PAYMENT\"].value_counts()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In the above chart, you can see that\u00a0`MISSED_PAYMENT`\u00a0is a binary variable with a fairly uniform distribution. This column is your target variable because whether a customer has missed a previous payment is the best indicator of whether they will miss a future payment."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Check the correlation between numerical features to understand relationships in the data"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "plt.figure(figsize=(16, 10))\ncorr = df.drop([\"CUSTOMER_ID\"], axis=1).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nheatmap = sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, annot=True, cbar_kws={\"shrink\": .5})\nheatmap.set_title('CORRELATION HEATMAP')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 3: Prepare the data\n\nTo prepare your data for model building, you can use data pre-processing techniques, including the addition, deletion, or transformation of training data. Here you\u2019ll set up the split between the training and testing data and transform some string data to numeric data to make it quantifiable. "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X = df.drop([\"CUSTOMER_ID\",\"MISSED_PAYMENT\"], axis=1)\ny = df[\"MISSED_PAYMENT\"]\n\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10, test_size=0.20)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "ordinal_cols_mapping = [{\n    \"col\":\"SMART_METER_COMMENTS\",    \n    \"mapping\": [('Positive', 1), ('Negative', -1), ('Neutral', 0)]\n}]\n\ncategorical_columns = X.drop([\"SMART_METER_COMMENTS\"], axis=1).select_dtypes(include='object').columns.tolist()\nnumerical_columns = X.select_dtypes(include=np.number).columns.tolist()\n\ncolumn_transformer = make_column_transformer(\n    (OneHotEncoder(handle_unknown='ignore'), categorical_columns), \n    (OrdinalEncoder(), [\"SMART_METER_COMMENTS\"]),\n    (MinMaxScaler(), numerical_columns), \n    remainder='passthrough')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 4: Build and train models\n\nTo find the best model, you\u2019ll train multiple candidate models. There are many predictive modeling algorithms to choose from. For this type of problem, these are the best choices:\n\n- Random Forest\n- Logistic Regression\n- XGBoost"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Random Forest"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "rf_pipeline = make_pipeline(column_transformer, RandomForestClassifier(n_estimators=100))\nrf_pipeline.fit(X_train, y_train)\n\ny_rf_score = rf_pipeline.score(X_test, y_test)\nprint(\"Random Forest model accuracy:\", np.round(y_rf_score, decimals=2))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Logistic Regression"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "lr_pipeline = make_pipeline(column_transformer, LogisticRegression())\nlr_pipeline.fit(X_train, y_train)\n\ny_lr_score = lr_pipeline.score(X_test, y_test)\nprint(\"Logistic Regression model accuracy:\", np.round(y_lr_score, decimals=2))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### XGBoost"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "xgb_pipeline = make_pipeline(column_transformer, XGBClassifier(use_label_encoder=False))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "xgb_pipeline.fit(X_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "y_xgb_score = xgb_pipeline.score(X_test, y_test)\nprint(\"XGBoost model accuracy:\", np.round(y_xgb_score, decimals=2))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 5: Evaluate the model\n\nNow you must evaluate your candidate models. A useful method for evaluating the performance of a model is measuring the area under the Receiver Operating Characteristic (ROC) curve. An ROC curve plots the true-positive rate (sensitivity) versus the false-positive rate (specificity). "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "y_rf_probs = rf_pipeline.predict_proba(X_test)[::,1]\ny_lr_probs = lr_pipeline.predict_proba(X_test)[::,1]\ny_xgb_probs = xgb_pipeline.predict_proba(X_test)[::,1]\n\nrf_auc = roc_auc_score(y_test, y_rf_probs)\nlr_auc = roc_auc_score(y_test, y_lr_probs)\nxgb_auc = roc_auc_score(y_test, y_xgb_probs)\n\nrf_fpr, rf_tpr, _ = roc_curve(y_test, y_rf_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, y_lr_probs)\nxgb_fpr, xgb_tpr, _ = roc_curve(y_test, y_xgb_probs)\n\nplt.figure(figsize=(8, 6))\n\nplt.plot([0,1],[0,1],'w--')\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (auc={:.1%})'.format(rf_auc))\nplt.plot(lr_fpr, lr_tpr, label='Logistic (auc={:.1%})'.format(lr_auc))\nplt.plot(xgb_fpr, xgb_tpr, label='XGBoost (auc={:.1%})'.format(xgb_auc))\n\nplt.title('ROC-AUC CURVES')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In the chart above, each line is color-coded by model. The legend shows that Random Forest has the highest ROC-AUC score, meaning it yields the best performance out of the three models. "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 6: Predict potential missed payments\n\nUse the Random Forest model to predict which of the 10 randomly selected customers from the original data set might miss a payment."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_sample = df.sample(n = 10)\ndf_sample.rename(columns={'MISSED_PAYMENT': 'Actual'}, inplace=True)\n\ny_proba = rf_pipeline.predict_proba(df_sample.drop([\"CUSTOMER_ID\",\"Actual\"], axis=1))\ndf_sample[\"Prediction\"] = labelencoder_y.inverse_transform(y_proba.argmax(axis=-1))\ndf_sample[\"Probability\"] = y_proba.max(axis=-1)\n\nprint(\"Predicting potential missed payments for 10 customers\")\ndf_sample[[\"CUSTOMER_ID\", \"Prediction\", \"Probability\"] + X.columns.tolist()]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The `Prediction` column contains the prediction results generated from the Random Forest model based on the customer payment history.\nThe `Probability` column contains the probability of each prediction."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Step 7: Deploy the model (optional)\n\nDeployment is the final stage of the lifecycle of a model or script. In a notebook, you can use the IBM Watson Machine Learning Python client library to deploy the trained machine learning model to IBM Watson Machine Learning.\n\nCheck out our online documentation, <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/wmls/wmls-deploy-overview.html\" target=\"_blank\" rel=\"noopener noreferrer\">Deploying assets</a>, for more samples, tutorials, and information.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Summary\n\nIn this notebook, you loaded and accessed the available data, prepared the data, and built a machine learning model to determine which clients should be offered payment plans."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Author\n\n**Eric Dong** is a Data Scientist at IBM."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "***\nCopyright \u00a9 IBM Corp. 2023. This notebook and its source code are released under the terms of the MIT License."
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}